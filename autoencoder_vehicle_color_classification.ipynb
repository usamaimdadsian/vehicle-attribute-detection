{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3asYa7_2IRp",
        "outputId": "fbbf9810-8933-4d9b-9301-553df60febff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  VeRi.zip\n",
            "replace ./VeRi/camera_Dist.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./VeRi/camera_ID.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/VeRi.zip ./\n",
        "!unzip VeRi.zip -d ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as  np\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from xml.etree import ElementTree as ET\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset,DataLoader,random_split"
      ],
      "metadata": {
        "id": "WjjnYJwb2Mbk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "tMgUfNX92OKS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,root_dir, xml_file,color_file, transform=None ) -> None:\n",
        "        self.root_dir = root_dir\n",
        "        self.color_mapping = self.load_color_mapping(color_file)\n",
        "        self.data = self.parse_xml(xml_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def load_color_mapping(self,color_file):\n",
        "        color_mapping = {}\n",
        "        with open(color_file,'r') as file:\n",
        "            for line in file:\n",
        "                color_id, color_name = line.strip().split(' ',1)\n",
        "                color_mapping[int(color_id)-1] = color_name\n",
        "        return color_mapping\n",
        "\n",
        "    def parse_xml(self, xml_file):\n",
        "        data = []\n",
        "        with open(xml_file,'r') as file:\n",
        "            tree = ET.fromstring(file.read())\n",
        "            tree = ET.ElementTree(tree)\n",
        "            root = tree.getroot()\n",
        "            for item in root.findall('.//Item'):\n",
        "                image_name = item.get('imageName')\n",
        "                color_id = int(item.get('colorID'))\n",
        "                data.append((image_name,color_id))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, color_id = self.data[idx]\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, color_id-1\n",
        "\n"
      ],
      "metadata": {
        "id": "6drgsFVX2Pw_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "            # Add more layers as needed\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.encoder_output_size(), 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def encoder_output_size(self):\n",
        "        encoded = self.encoder(torch.empty(1,3,SIZE,SIZE))\n",
        "        return encoded.view(encoded.size(0), -1).size()[-1]\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        classification = self.classifier(encoded.view(encoded.size(0), -1))\n",
        "        return decoded, classification\n"
      ],
      "metadata": {
        "id": "fbowUo4i2URQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def train(epochs, model, criterion_autoencoder , criterion_classifier, optimizer, train_loader, valid_loader, test_loader):\n",
        "  best_model = None\n",
        "  best_acc = 0\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_train_correct = 0\n",
        "      total_train_samples = 0\n",
        "      for images, labels in train_loader:\n",
        "          images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs_autoencoder, outputs_classifier = model(images)\n",
        "\n",
        "          loss_autoencoder = criterion_autoencoder(outputs_autoencoder, images)\n",
        "          loss_classifier = criterion_classifier(outputs_classifier, labels)\n",
        "\n",
        "          total_loss = loss_autoencoder + 0.1 * loss_classifier\n",
        "\n",
        "          total_loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          _, predicted = torch.max(outputs_classifier, 1)\n",
        "          total_train_correct += (predicted == labels).sum().item()\n",
        "          total_train_samples += labels.size(0)\n",
        "      train_accuracy = total_train_correct / total_train_samples\n",
        "\n",
        "      # Validation loop\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          total_correct = 0\n",
        "          total_samples = 0\n",
        "          for images, labels in valid_loader:\n",
        "              images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "              _,outputs = model(images)\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "              total_correct += (predicted == labels).sum().item()\n",
        "              total_samples += labels.size(0)\n",
        "\n",
        "          accuracy = total_correct / total_samples\n",
        "          print(f'Epoch [{epoch+1}/{epochs}], Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "          if (accuracy > best_acc):\n",
        "            print(\"New Best Model with Accuracy: \", accuracy)\n",
        "            best_acc = accuracy\n",
        "            best_model = copy.deepcopy(model)\n",
        "\n",
        "  print(\"Training finished.\")\n",
        "\n",
        "  # Testing the model\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      total_correct = 0\n",
        "      total_samples = 0\n",
        "      for images, labels in test_loader:\n",
        "          images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "          _,outputs = best_model(images)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total_correct += (predicted == labels).sum().item()\n",
        "          total_samples += labels.size(0)\n",
        "\n",
        "      accuracy = total_correct / total_samples\n",
        "      print(f'Test Accuracy On best Model: {accuracy:.4f}')\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "wHncojEg3VKL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = CustomDataset(\"VeRi/image_train\",\"VeRi/train_label.xml\",\"VeRi/list_color.txt\",transform)\n",
        "\n",
        "train_size = int(0.8*len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(dataset,[train_size,val_size])\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = CustomDataset(\"VeRi/image_test\",\"VeRi/test_label.xml\",\"VeRi/list_color.txt\",transform)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 10\n",
        "SIZE = 224\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "-7JFXcyO4-U9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = Autoencoder().to(DEVICE)\n",
        "classifier_criterion = nn.CrossEntropyLoss()\n",
        "autoencoder_criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "best_model = train(EPOCHS,model,autoencoder_criterion , classifier_criterion, optimizer, train_loader, valid_loader, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u74EkHnB5HcW",
        "outputId": "a94753d0-6b62-477b-cef1-7dd239856749"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Training Accuracy: 0.7830, Validation Accuracy: 0.8869\n",
            "New Best Model with Accuracy:  0.886887417218543\n",
            "Epoch [2/10], Training Accuracy: 0.9164, Validation Accuracy: 0.9163\n",
            "New Best Model with Accuracy:  0.9162913907284769\n",
            "Epoch [3/10], Training Accuracy: 0.9629, Validation Accuracy: 0.9339\n",
            "New Best Model with Accuracy:  0.9339072847682119\n",
            "Epoch [4/10], Training Accuracy: 0.9740, Validation Accuracy: 0.9225\n",
            "Epoch [5/10], Training Accuracy: 0.9858, Validation Accuracy: 0.9146\n",
            "Epoch [6/10], Training Accuracy: 0.9852, Validation Accuracy: 0.9385\n",
            "New Best Model with Accuracy:  0.9385430463576159\n",
            "Epoch [7/10], Training Accuracy: 0.9872, Validation Accuracy: 0.9440\n",
            "New Best Model with Accuracy:  0.9439735099337748\n",
            "Epoch [8/10], Training Accuracy: 0.9905, Validation Accuracy: 0.9066\n",
            "Epoch [9/10], Training Accuracy: 0.9881, Validation Accuracy: 0.9375\n",
            "Epoch [10/10], Training Accuracy: 0.9916, Validation Accuracy: 0.9287\n",
            "Training finished.\n",
            "Test Accuracy On best Model: 0.8726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model,\"vehicle_color_classification.pt\")"
      ],
      "metadata": {
        "id": "RpTBqbynD102"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}